{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Huber Regression\n",
    "This notebook illustrates how to use LightGBM to optimize the Generalized Huber loss (GHL) function with a log link function. Please ensure that you have LightGBM and scikit-learn installed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import skewnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we consider a one dimensional linear problem of N data points with a skew normal error distribution and non-constant variance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "sigma0 = 1\n",
    "xvec = np.linspace(-500, 500, N)\n",
    "yvec = np.array([\n",
    "    x + skewnorm.rvs(a=100, loc=0, scale=(sigma0 + np.abs(x)**(2)))\n",
    "    for x in xvec\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the log link function and it's inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_link_trans(x):\n",
    "    bool1 = x < 0\n",
    "    bool2 = x >= 0\n",
    "    x_trans = np.zeros([1, len(x)]).flatten()\n",
    "    x_trans[bool1] = -np.log1p(-x[bool1])\n",
    "    x_trans[bool2] = np.log1p(x[bool2])\n",
    "    return x_trans\n",
    "\n",
    "\n",
    "def log_link_back_trans(x):\n",
    "    bool1 = x < 0\n",
    "    bool2 = x >= 0\n",
    "    x_back_trans = np.zeros([1, len(x)]).flatten()\n",
    "    x_back_trans[bool1] = (1 - np.exp(-x[bool1]))\n",
    "    x_back_trans[bool2] = (np.exp(x[bool2]) - 1)\n",
    "    return x_back_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define 2 metric functions which both compute the $r^{2}$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric_func_1(yhat, dtrain):\n",
    "    y = dtrain.get_label().flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    yhat_back_trans = log_link_back_trans(yhat)\n",
    "    y_back_trans = log_link_back_trans(y)\n",
    "    val = r2_score(y_back_trans, yhat_back_trans)\n",
    "    return ('r2_score', val, True)\n",
    "\n",
    "\n",
    "def metric_func_2(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    yhat = yhat.flatten()\n",
    "    yhat_back_trans = log_link_back_trans(yhat)\n",
    "    val = r2_score(y, yhat_back_trans)\n",
    "    return ('r2_score', val, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines the objective function for LightGBM by computing the 1st and 2nd derivative of the GHL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generalized_huber_obj(yhat, dtrain, alpha=0.75):\n",
    "    y = dtrain.get_label()\n",
    "    yhat = yhat.flatten()\n",
    "\n",
    "    def sgn(x):\n",
    "        sig = np.sign(x)\n",
    "        sig[sig == 0] = 1\n",
    "        return sig\n",
    "\n",
    "    g = lambda x: sgn(x) * np.log(1 + np.abs(x))\n",
    "    ginv = lambda x: sgn(x) * (np.exp(np.abs(x)) - 1)\n",
    "    ginvp = lambda x: np.exp(np.abs(x))\n",
    "    ginvpp = lambda x: sgn(x) * np.exp(np.abs(x))\n",
    "\n",
    "    diff = g(y) - yhat\n",
    "    absdiff = np.abs(diff)\n",
    "\n",
    "    bool1_l = ((absdiff <= alpha) & (diff < 0))\n",
    "    bool1_r = ((absdiff <= alpha) & (diff >= 0))\n",
    "    bool2_l = ((absdiff > alpha) & (diff < 0))\n",
    "    bool2_r = ((absdiff > alpha) & (diff >= 0))\n",
    "\n",
    "    grad = np.zeros([1, len(yhat)]).flatten()\n",
    "    hess = np.zeros([1, len(yhat)]).flatten()\n",
    "\n",
    "    A = np.zeros([1, len(yhat)]).flatten()\n",
    "    Ap = np.zeros([1, len(yhat)]).flatten()\n",
    "    App = np.zeros([1, len(yhat)]).flatten()\n",
    "    B = np.zeros([1, len(yhat)]).flatten()\n",
    "\n",
    "    A[bool1_l] = ginv(yhat[bool1_l] - alpha) - ginv(yhat[bool1_l])\n",
    "    A[bool1_r] = ginv(yhat[bool1_r] + alpha) - ginv(yhat[bool1_r])\n",
    "    Ap[bool1_l] = ginvp(yhat[bool1_l] - alpha) - ginvp(yhat[bool1_l])\n",
    "    Ap[bool1_r] = ginvp(yhat[bool1_r] + alpha) - ginvp(yhat[bool1_r])\n",
    "    App[bool1_l] = ginvpp(yhat[bool1_l] - alpha) - ginvpp(yhat[bool1_l])\n",
    "    App[bool1_r] = ginvpp(yhat[bool1_r] + alpha) - ginvpp(yhat[bool1_r])\n",
    "\n",
    "    A[bool2_l] = ginv(yhat[bool2_l] - alpha) - ginv(yhat[bool2_l])\n",
    "    A[bool2_r] = ginv(yhat[bool2_r] + alpha) - ginv(yhat[bool2_r])\n",
    "    Ap[bool2_l] = ginvp(yhat[bool2_l] - alpha) - ginvp(yhat[bool2_l])\n",
    "    Ap[bool2_r] = ginvp(yhat[bool2_r] + alpha) - ginvp(yhat[bool2_r])\n",
    "    App[bool2_l] = ginvpp(yhat[bool2_l] - alpha) - ginvpp(yhat[bool2_l])\n",
    "    App[bool2_r] = ginvpp(yhat[bool2_r] + alpha) - ginvpp(yhat[bool2_r])\n",
    "\n",
    "    B[bool1_l] = y[bool1_l] - ginv(g(y[bool1_l]) + alpha)\n",
    "    B[bool1_r] = y[bool1_r] - ginv(g(y[bool1_r]) - alpha)\n",
    "\n",
    "    grad[bool1_l] = -2*(y[bool1_l]-ginv(yhat[bool1_l]))*ginvp(yhat[bool1_l])*(1/np.abs(A[bool1_l]) + 1/np.abs(B[bool1_l])) \\\n",
    "                    -(y[bool1_l]-ginv(yhat[bool1_l]))**2*(1/(np.abs(A[bool1_l])**2))*sgn(A[bool1_l])*Ap[bool1_l]\n",
    "\n",
    "    grad[bool1_r] = -2*(y[bool1_r]-ginv(yhat[bool1_r]))*ginvp(yhat[bool1_r])*(1/np.abs(A[bool1_r]) + 1/np.abs(B[bool1_r])) \\\n",
    "                    -(y[bool1_r]-ginv(yhat[bool1_r]))**2*(1/(np.abs(A[bool1_r])**2))*sgn(A[bool1_r])*Ap[bool1_r]\n",
    "\n",
    "    hess[bool1_l] = 2*(ginvp(yhat[bool1_l])**2 - (y[bool1_l]-ginv(yhat[bool1_l]))*ginvpp(yhat[bool1_l]))*(1/np.abs(A[bool1_l]) + 1/np.abs(B[bool1_l])) \\\n",
    "                    +4*(y[bool1_l]-ginv(yhat[bool1_l]))*ginvp(yhat[bool1_l])*(1/(np.abs(A[bool1_l])**2))*sgn(A[bool1_l])*Ap[bool1_l] \\\n",
    "                    +2*(y[bool1_l]-ginv(yhat[bool1_l]))**2*(1/(np.abs(A[bool1_l])**3))*Ap[bool1_l]**2 \\\n",
    "                    -(y[bool1_l]-ginv(yhat[bool1_l]))**2*(1/(np.abs(A[bool1_l])**2))*sgn(A[bool1_l])*App[bool1_l]\n",
    "\n",
    "    hess[bool1_r] = 2*(ginvp(yhat[bool1_r])**2 - (y[bool1_r]-ginv(yhat[bool1_r]))*ginvpp(yhat[bool1_r]))*(1/np.abs(A[bool1_r]) + 1/np.abs(B[bool1_r])) \\\n",
    "                   +4*(y[bool1_r]-ginv(yhat[bool1_r]))*ginvp(yhat[bool1_r])*(1/(np.abs(A[bool1_r])**2))*sgn(A[bool1_r])*Ap[bool1_r] \\\n",
    "                   +2*(y[bool1_r]-ginv(yhat[bool1_r]))**2*(1/(np.abs(A[bool1_r])**3))*Ap[bool1_r]**2 \\\n",
    "                    -(y[bool1_r]-ginv(yhat[bool1_r]))**2*(1/(np.abs(A[bool1_r])**2))*sgn(A[bool1_r])*App[bool1_r]\n",
    "\n",
    "    grad[bool2_l] = -4 * sgn(y[bool2_l] - ginv(yhat[bool2_l])) * ginvp(\n",
    "        yhat[bool2_l]) - sgn(A[bool2_l]) * Ap[bool2_l]\n",
    "\n",
    "    grad[bool2_r] = -4 * sgn(y[bool2_r] - ginv(yhat[bool2_r])) * ginvp(\n",
    "        yhat[bool2_r]) - sgn(A[bool2_r]) * Ap[bool2_r]\n",
    "\n",
    "    hess[bool2_l] = -4 * sgn(y[bool2_l] - ginv(yhat[bool2_l])) * ginvpp(\n",
    "        yhat[bool2_l]) - sgn(A[bool2_l]) * App[bool2_l]\n",
    "\n",
    "    hess[bool2_r] = -4 * sgn(y[bool2_r] - ginv(yhat[bool2_r])) * ginvpp(\n",
    "        yhat[bool2_r]) - sgn(A[bool2_r]) * App[bool2_r]\n",
    "\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we put our data into a Dataframe, define the train and test sets and compute 3 cv folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.column_stack((xvec, yvec)), columns=['x', 'y'])\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.3)\n",
    "y_train = X_train['y']\n",
    "y_test = X_test['y']\n",
    "\n",
    "groups = X_train.index\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "group_kfold.get_n_splits(X_train, groups)\n",
    "folds = list(group_kfold.split(X_train, y_train, groups=groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by training a shallow Lightgbm model whose output will be the starting vector of the GHL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_param_start_vector = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1,\n",
    "    'alpha': 2,\n",
    "    'objective': 'huber',\n",
    "    'metric': 'none',\n",
    "    'tree_learner': 'data',\n",
    "    'verbosity': 3\n",
    "}\n",
    "\n",
    "lgbtrain_trans = lgb.Dataset(\n",
    "    X_train.drop('y', axis=1), label=log_link_trans(y_train))\n",
    "lgbmodel0 = lgb.train(\n",
    "    lgb_param_start_vector, lgbtrain_trans, num_boost_round=20)\n",
    "\n",
    "start_vec = lgbmodel0.predict(X_train.drop('y', axis=1))\n",
    "lgbtrain = lgb.Dataset(\n",
    "    X_train.drop('y', axis=1), init_score=start_vec, label=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform a hyperparameter search to find the optimal $\\alpha$ parameter in the GHL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.147368421053\n",
      "0.194736842105\n",
      "0.242105263158\n",
      "0.289473684211\n",
      "0.336842105263\n",
      "0.384210526316\n",
      "0.431578947368\n",
      "0.478947368421\n",
      "0.526315789474\n",
      "0.573684210526\n",
      "0.621052631579\n",
      "0.668421052632\n",
      "0.715789473684\n",
      "0.763157894737\n",
      "0.810526315789\n",
      "0.857894736842\n",
      "0.905263157895\n",
      "0.952631578947\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "lgb_param_ghl = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1,\n",
    "    'metric': 'none',\n",
    "    'max_delta_step': 3,\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "kwargsvec = np.linspace(0.1, 1, 20)\n",
    "chvec = np.zeros(20)\n",
    "for k in range(20):\n",
    "    kwargs = {'alpha': kwargsvec[k]}\n",
    "    cvmodel = lgb.cv(\n",
    "        lgb_param_ghl,\n",
    "        lgbtrain,\n",
    "        num_boost_round=10000,\n",
    "        folds=folds,\n",
    "        fobj=(lambda a, b: generalized_huber_obj(a, b, **kwargs)),\n",
    "        metrics='none',\n",
    "        feval=metric_func_2,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=False)\n",
    "    chvec[k] = cvmodel['r2_score-mean'][-1]\n",
    "    print(kwargsvec[k])\n",
    "\n",
    "best_iter = np.argmax(chvec)\n",
    "alpha_ch = kwargsvec[best_iter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having found the optimal $\\alpha$ we can now train the GHL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's r2_score: 0.315028 + 0.000139703\n",
      "[2]\tcv_agg's r2_score: 0.342772 + 0.000454857\n",
      "[3]\tcv_agg's r2_score: 0.361445 + 0.000666957\n",
      "[4]\tcv_agg's r2_score: 0.37504 + 0.000807633\n",
      "[5]\tcv_agg's r2_score: 0.385405 + 0.000917626\n",
      "[6]\tcv_agg's r2_score: 0.393495 + 0.000991963\n",
      "[7]\tcv_agg's r2_score: 0.399925 + 0.00104371\n",
      "[8]\tcv_agg's r2_score: 0.40511 + 0.00108262\n",
      "[9]\tcv_agg's r2_score: 0.409341 + 0.00111595\n",
      "[10]\tcv_agg's r2_score: 0.412822 + 0.00114632\n",
      "[11]\tcv_agg's r2_score: 0.41571 + 0.00116981\n",
      "[12]\tcv_agg's r2_score: 0.418119 + 0.00119227\n",
      "[13]\tcv_agg's r2_score: 0.420144 + 0.00120966\n",
      "[14]\tcv_agg's r2_score: 0.421857 + 0.00122312\n",
      "[15]\tcv_agg's r2_score: 0.42331 + 0.00123593\n",
      "[16]\tcv_agg's r2_score: 0.424551 + 0.00124597\n",
      "[17]\tcv_agg's r2_score: 0.425614 + 0.0012578\n",
      "[18]\tcv_agg's r2_score: 0.426531 + 0.00126598\n",
      "[19]\tcv_agg's r2_score: 0.427323 + 0.00127319\n",
      "[20]\tcv_agg's r2_score: 0.428007 + 0.00127968\n",
      "[21]\tcv_agg's r2_score: 0.428602 + 0.00128593\n",
      "[22]\tcv_agg's r2_score: 0.429119 + 0.00129259\n",
      "[23]\tcv_agg's r2_score: 0.429572 + 0.00129767\n",
      "[24]\tcv_agg's r2_score: 0.429971 + 0.00130191\n",
      "[25]\tcv_agg's r2_score: 0.430319 + 0.0013058\n",
      "[26]\tcv_agg's r2_score: 0.430627 + 0.00131033\n",
      "[27]\tcv_agg's r2_score: 0.430899 + 0.00131231\n",
      "[28]\tcv_agg's r2_score: 0.431137 + 0.00131535\n",
      "[29]\tcv_agg's r2_score: 0.431349 + 0.00131799\n",
      "[30]\tcv_agg's r2_score: 0.431537 + 0.00132081\n",
      "[31]\tcv_agg's r2_score: 0.431703 + 0.00132362\n",
      "[32]\tcv_agg's r2_score: 0.431851 + 0.00132469\n",
      "[33]\tcv_agg's r2_score: 0.431982 + 0.00132574\n",
      "[34]\tcv_agg's r2_score: 0.432099 + 0.00132737\n",
      "[35]\tcv_agg's r2_score: 0.432202 + 0.00132829\n",
      "[36]\tcv_agg's r2_score: 0.432294 + 0.00133039\n",
      "[37]\tcv_agg's r2_score: 0.432377 + 0.0013306\n",
      "[38]\tcv_agg's r2_score: 0.43245 + 0.00133105\n",
      "[39]\tcv_agg's r2_score: 0.432515 + 0.00133156\n",
      "[40]\tcv_agg's r2_score: 0.432573 + 0.00133329\n",
      "[41]\tcv_agg's r2_score: 0.432625 + 0.00133204\n",
      "[42]\tcv_agg's r2_score: 0.43267 + 0.00133208\n",
      "[43]\tcv_agg's r2_score: 0.432713 + 0.00133193\n",
      "[44]\tcv_agg's r2_score: 0.43275 + 0.00133322\n",
      "[45]\tcv_agg's r2_score: 0.432784 + 0.00133293\n",
      "[46]\tcv_agg's r2_score: 0.432813 + 0.00133403\n",
      "[47]\tcv_agg's r2_score: 0.432839 + 0.00133341\n",
      "[48]\tcv_agg's r2_score: 0.432862 + 0.00133457\n",
      "[49]\tcv_agg's r2_score: 0.432884 + 0.00133416\n",
      "[50]\tcv_agg's r2_score: 0.432902 + 0.00133394\n",
      "[51]\tcv_agg's r2_score: 0.432919 + 0.00133434\n",
      "[52]\tcv_agg's r2_score: 0.432934 + 0.00133482\n",
      "[53]\tcv_agg's r2_score: 0.432948 + 0.00133336\n",
      "[54]\tcv_agg's r2_score: 0.432959 + 0.00133322\n",
      "[55]\tcv_agg's r2_score: 0.432971 + 0.0013328\n",
      "[56]\tcv_agg's r2_score: 0.432981 + 0.00133322\n",
      "[57]\tcv_agg's r2_score: 0.43299 + 0.00133362\n",
      "[58]\tcv_agg's r2_score: 0.432998 + 0.00133344\n",
      "[59]\tcv_agg's r2_score: 0.433005 + 0.00133397\n",
      "[60]\tcv_agg's r2_score: 0.43301 + 0.00133379\n",
      "[61]\tcv_agg's r2_score: 0.433015 + 0.00133351\n",
      "[62]\tcv_agg's r2_score: 0.43302 + 0.00133323\n",
      "[63]\tcv_agg's r2_score: 0.433024 + 0.00133344\n",
      "[64]\tcv_agg's r2_score: 0.433028 + 0.00133371\n",
      "[65]\tcv_agg's r2_score: 0.433031 + 0.00133288\n",
      "[66]\tcv_agg's r2_score: 0.433034 + 0.00133289\n",
      "[67]\tcv_agg's r2_score: 0.433038 + 0.00133323\n",
      "[68]\tcv_agg's r2_score: 0.433039 + 0.00133345\n",
      "[69]\tcv_agg's r2_score: 0.433041 + 0.00133343\n",
      "[70]\tcv_agg's r2_score: 0.433043 + 0.00133301\n",
      "[71]\tcv_agg's r2_score: 0.433045 + 0.00133249\n",
      "[72]\tcv_agg's r2_score: 0.433046 + 0.00133217\n",
      "[73]\tcv_agg's r2_score: 0.433048 + 0.00133238\n",
      "[74]\tcv_agg's r2_score: 0.433048 + 0.00133253\n",
      "[75]\tcv_agg's r2_score: 0.433049 + 0.001333\n",
      "[76]\tcv_agg's r2_score: 0.43305 + 0.00133292\n",
      "[77]\tcv_agg's r2_score: 0.433051 + 0.0013328\n",
      "[78]\tcv_agg's r2_score: 0.433051 + 0.00133306\n",
      "[79]\tcv_agg's r2_score: 0.433051 + 0.00133305\n",
      "[80]\tcv_agg's r2_score: 0.433052 + 0.00133336\n",
      "[81]\tcv_agg's r2_score: 0.433052 + 0.0013332\n",
      "[82]\tcv_agg's r2_score: 0.433052 + 0.00133317\n",
      "[83]\tcv_agg's r2_score: 0.433052 + 0.00133319\n",
      "[84]\tcv_agg's r2_score: 0.433052 + 0.00133271\n",
      "[85]\tcv_agg's r2_score: 0.433052 + 0.00133219\n",
      "[86]\tcv_agg's r2_score: 0.433052 + 0.00133232\n",
      "[87]\tcv_agg's r2_score: 0.433052 + 0.00133239\n",
      "[88]\tcv_agg's r2_score: 0.433052 + 0.00133228\n",
      "[89]\tcv_agg's r2_score: 0.433052 + 0.00133194\n",
      "[90]\tcv_agg's r2_score: 0.433052 + 0.00133241\n",
      "[91]\tcv_agg's r2_score: 0.433051 + 0.0013323\n",
      "[92]\tcv_agg's r2_score: 0.433052 + 0.0013322\n",
      "[93]\tcv_agg's r2_score: 0.433051 + 0.00133206\n",
      "[94]\tcv_agg's r2_score: 0.433051 + 0.00133215\n",
      "[95]\tcv_agg's r2_score: 0.433051 + 0.00133214\n",
      "[96]\tcv_agg's r2_score: 0.433051 + 0.001332\n",
      "[97]\tcv_agg's r2_score: 0.43305 + 0.00133233\n",
      "[98]\tcv_agg's r2_score: 0.43305 + 0.00133229\n",
      "[99]\tcv_agg's r2_score: 0.433049 + 0.0013322\n",
      "[100]\tcv_agg's r2_score: 0.433049 + 0.00133235\n",
      "[101]\tcv_agg's r2_score: 0.433049 + 0.00133225\n",
      "[102]\tcv_agg's r2_score: 0.433048 + 0.00133212\n",
      "[103]\tcv_agg's r2_score: 0.433048 + 0.00133205\n",
      "[104]\tcv_agg's r2_score: 0.433048 + 0.00133168\n",
      "[105]\tcv_agg's r2_score: 0.433047 + 0.00133173\n",
      "[106]\tcv_agg's r2_score: 0.433047 + 0.00133155\n",
      "generalized huber best alpha:  0.668421052632\n",
      "generalized huber best iteration:  86\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'alpha': alpha_ch}\n",
    "cvmodel = lgb.cv(\n",
    "    lgb_param_ghl,\n",
    "    lgbtrain,\n",
    "    num_boost_round=10000,\n",
    "    folds=folds,\n",
    "    fobj=(lambda a, b: generalized_huber_obj(a, b, **kwargs)),\n",
    "    metrics='none',\n",
    "    feval=metric_func_2,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=True)\n",
    "\n",
    "print('generalized huber best alpha: ', alpha_ch)\n",
    "print('generalized huber best iteration: ', len(cvmodel['r2_score-mean']))\n",
    "\n",
    "lgbmodel = lgb.train(\n",
    "    lgb_param_ghl,\n",
    "    lgbtrain,\n",
    "    num_boost_round=len(cvmodel['r2_score-mean']),\n",
    "    fobj=(lambda a, b: generalized_huber_obj(a, b, **kwargs)),\n",
    "    feval=metric_func_2,\n",
    "    verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next a MAE model is trained on $\\log(y)$ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's r2_score: -0.110404 + 0.000665364\n",
      "[2]\tcv_agg's r2_score: -0.0615394 + 0.000558223\n",
      "[3]\tcv_agg's r2_score: -0.0163966 + 0.000508024\n",
      "[4]\tcv_agg's r2_score: 0.0256555 + 0.000535871\n",
      "[5]\tcv_agg's r2_score: 0.0648368 + 0.000601036\n",
      "[6]\tcv_agg's r2_score: 0.101102 + 0.000615428\n",
      "[7]\tcv_agg's r2_score: 0.134598 + 0.000661443\n",
      "[8]\tcv_agg's r2_score: 0.165353 + 0.00068997\n",
      "[9]\tcv_agg's r2_score: 0.193312 + 0.000683287\n",
      "[10]\tcv_agg's r2_score: 0.21857 + 0.000695577\n",
      "[11]\tcv_agg's r2_score: 0.24124 + 0.000712882\n",
      "[12]\tcv_agg's r2_score: 0.261572 + 0.000753481\n",
      "[13]\tcv_agg's r2_score: 0.279689 + 0.000762501\n",
      "[14]\tcv_agg's r2_score: 0.295759 + 0.000771255\n",
      "[15]\tcv_agg's r2_score: 0.309956 + 0.000768499\n",
      "[16]\tcv_agg's r2_score: 0.322511 + 0.000808143\n",
      "[17]\tcv_agg's r2_score: 0.333585 + 0.000824778\n",
      "[18]\tcv_agg's r2_score: 0.343312 + 0.000814279\n",
      "[19]\tcv_agg's r2_score: 0.351867 + 0.000838262\n",
      "[20]\tcv_agg's r2_score: 0.359355 + 0.00083988\n",
      "[21]\tcv_agg's r2_score: 0.365936 + 0.000838567\n",
      "[22]\tcv_agg's r2_score: 0.371716 + 0.000845629\n",
      "[23]\tcv_agg's r2_score: 0.376768 + 0.000846138\n",
      "[24]\tcv_agg's r2_score: 0.381236 + 0.000838173\n",
      "[25]\tcv_agg's r2_score: 0.385163 + 0.000840831\n",
      "[26]\tcv_agg's r2_score: 0.388591 + 0.000843322\n",
      "[27]\tcv_agg's r2_score: 0.39159 + 0.000819402\n",
      "[28]\tcv_agg's r2_score: 0.394241 + 0.000828131\n",
      "[29]\tcv_agg's r2_score: 0.396576 + 0.000821572\n",
      "[30]\tcv_agg's r2_score: 0.398638 + 0.000823487\n",
      "[31]\tcv_agg's r2_score: 0.400477 + 0.000823548\n",
      "[32]\tcv_agg's r2_score: 0.402096 + 0.000821809\n",
      "[33]\tcv_agg's r2_score: 0.403536 + 0.000817114\n",
      "[34]\tcv_agg's r2_score: 0.404824 + 0.000841872\n",
      "[35]\tcv_agg's r2_score: 0.405964 + 0.000849704\n",
      "[36]\tcv_agg's r2_score: 0.406975 + 0.000846455\n",
      "[37]\tcv_agg's r2_score: 0.407868 + 0.000842414\n",
      "[38]\tcv_agg's r2_score: 0.408678 + 0.000864101\n",
      "[39]\tcv_agg's r2_score: 0.409389 + 0.00086395\n",
      "[40]\tcv_agg's r2_score: 0.410039 + 0.000874193\n",
      "[41]\tcv_agg's r2_score: 0.410606 + 0.000882485\n",
      "[42]\tcv_agg's r2_score: 0.411126 + 0.000893836\n",
      "[43]\tcv_agg's r2_score: 0.411589 + 0.000896734\n",
      "[44]\tcv_agg's r2_score: 0.412005 + 0.000894299\n",
      "[45]\tcv_agg's r2_score: 0.41237 + 0.000897304\n",
      "[46]\tcv_agg's r2_score: 0.412701 + 0.000903878\n",
      "[47]\tcv_agg's r2_score: 0.412984 + 0.000905505\n",
      "[48]\tcv_agg's r2_score: 0.41324 + 0.000897397\n",
      "[49]\tcv_agg's r2_score: 0.413472 + 0.000905852\n",
      "[50]\tcv_agg's r2_score: 0.413683 + 0.000910059\n",
      "[51]\tcv_agg's r2_score: 0.413874 + 0.000909079\n",
      "[52]\tcv_agg's r2_score: 0.414043 + 0.000906785\n",
      "[53]\tcv_agg's r2_score: 0.414197 + 0.000906393\n",
      "[54]\tcv_agg's r2_score: 0.414336 + 0.000906754\n",
      "[55]\tcv_agg's r2_score: 0.41446 + 0.000915657\n",
      "[56]\tcv_agg's r2_score: 0.414567 + 0.000923393\n",
      "[57]\tcv_agg's r2_score: 0.414665 + 0.000923816\n",
      "[58]\tcv_agg's r2_score: 0.414747 + 0.000924999\n",
      "[59]\tcv_agg's r2_score: 0.414826 + 0.00092383\n",
      "[60]\tcv_agg's r2_score: 0.414901 + 0.000919819\n",
      "[61]\tcv_agg's r2_score: 0.414967 + 0.000922816\n",
      "[62]\tcv_agg's r2_score: 0.415025 + 0.000926128\n",
      "[63]\tcv_agg's r2_score: 0.415076 + 0.00092003\n",
      "[64]\tcv_agg's r2_score: 0.415122 + 0.00092268\n",
      "[65]\tcv_agg's r2_score: 0.415161 + 0.000921346\n",
      "[66]\tcv_agg's r2_score: 0.415199 + 0.000924133\n",
      "[67]\tcv_agg's r2_score: 0.415238 + 0.000926386\n",
      "[68]\tcv_agg's r2_score: 0.415267 + 0.000924289\n",
      "[69]\tcv_agg's r2_score: 0.415297 + 0.00091921\n",
      "[70]\tcv_agg's r2_score: 0.415323 + 0.000918015\n",
      "[71]\tcv_agg's r2_score: 0.415345 + 0.0009202\n",
      "[72]\tcv_agg's r2_score: 0.415371 + 0.000917668\n",
      "[73]\tcv_agg's r2_score: 0.415394 + 0.000916274\n",
      "[74]\tcv_agg's r2_score: 0.415413 + 0.000911727\n",
      "[75]\tcv_agg's r2_score: 0.415432 + 0.000909897\n",
      "[76]\tcv_agg's r2_score: 0.415442 + 0.000910184\n",
      "[77]\tcv_agg's r2_score: 0.41546 + 0.000916458\n",
      "[78]\tcv_agg's r2_score: 0.415475 + 0.000912983\n",
      "[79]\tcv_agg's r2_score: 0.415492 + 0.00091474\n",
      "[80]\tcv_agg's r2_score: 0.4155 + 0.000914233\n",
      "[81]\tcv_agg's r2_score: 0.415511 + 0.000908667\n",
      "[82]\tcv_agg's r2_score: 0.415518 + 0.000905949\n",
      "[83]\tcv_agg's r2_score: 0.415526 + 0.000908477\n",
      "[84]\tcv_agg's r2_score: 0.415533 + 0.000910674\n",
      "[85]\tcv_agg's r2_score: 0.415542 + 0.000907647\n",
      "[86]\tcv_agg's r2_score: 0.415551 + 0.000906103\n",
      "[87]\tcv_agg's r2_score: 0.415558 + 0.000906559\n",
      "[88]\tcv_agg's r2_score: 0.415566 + 0.000911443\n",
      "[89]\tcv_agg's r2_score: 0.41557 + 0.000912254\n",
      "[90]\tcv_agg's r2_score: 0.415575 + 0.000913315\n",
      "[91]\tcv_agg's r2_score: 0.415581 + 0.000914618\n",
      "[92]\tcv_agg's r2_score: 0.415587 + 0.000916513\n",
      "[93]\tcv_agg's r2_score: 0.41559 + 0.000919183\n",
      "[94]\tcv_agg's r2_score: 0.415594 + 0.000918508\n",
      "[95]\tcv_agg's r2_score: 0.415599 + 0.000915915\n",
      "[96]\tcv_agg's r2_score: 0.415604 + 0.000910795\n",
      "[97]\tcv_agg's r2_score: 0.415604 + 0.000909346\n",
      "[98]\tcv_agg's r2_score: 0.415609 + 0.000912892\n",
      "[99]\tcv_agg's r2_score: 0.415612 + 0.000916076\n",
      "[100]\tcv_agg's r2_score: 0.415614 + 0.000916328\n",
      "[101]\tcv_agg's r2_score: 0.415614 + 0.000913109\n",
      "[102]\tcv_agg's r2_score: 0.415615 + 0.000913023\n",
      "[103]\tcv_agg's r2_score: 0.415615 + 0.000911268\n",
      "[104]\tcv_agg's r2_score: 0.415617 + 0.000908806\n",
      "[105]\tcv_agg's r2_score: 0.415619 + 0.000911277\n",
      "[106]\tcv_agg's r2_score: 0.415621 + 0.000911118\n",
      "[107]\tcv_agg's r2_score: 0.415621 + 0.000912188\n",
      "[108]\tcv_agg's r2_score: 0.415624 + 0.000909551\n",
      "[109]\tcv_agg's r2_score: 0.415627 + 0.000909392\n",
      "[110]\tcv_agg's r2_score: 0.415628 + 0.000910453\n",
      "[111]\tcv_agg's r2_score: 0.415628 + 0.000910799\n",
      "[112]\tcv_agg's r2_score: 0.415628 + 0.000906789\n",
      "[113]\tcv_agg's r2_score: 0.415627 + 0.000905003\n",
      "[114]\tcv_agg's r2_score: 0.415626 + 0.000904176\n",
      "[115]\tcv_agg's r2_score: 0.415626 + 0.000901739\n",
      "[116]\tcv_agg's r2_score: 0.415625 + 0.000901525\n",
      "[117]\tcv_agg's r2_score: 0.415624 + 0.00090051\n",
      "[118]\tcv_agg's r2_score: 0.415623 + 0.000900497\n",
      "[119]\tcv_agg's r2_score: 0.415624 + 0.000901708\n",
      "[120]\tcv_agg's r2_score: 0.415623 + 0.000900903\n",
      "[121]\tcv_agg's r2_score: 0.415623 + 0.000900678\n",
      "[122]\tcv_agg's r2_score: 0.415623 + 0.000900158\n",
      "[123]\tcv_agg's r2_score: 0.415623 + 0.000899021\n",
      "[124]\tcv_agg's r2_score: 0.415623 + 0.000898216\n",
      "[125]\tcv_agg's r2_score: 0.415625 + 0.00089871\n",
      "[126]\tcv_agg's r2_score: 0.415626 + 0.000900123\n",
      "[127]\tcv_agg's r2_score: 0.415626 + 0.000899544\n",
      "[128]\tcv_agg's r2_score: 0.415625 + 0.000898973\n",
      "[129]\tcv_agg's r2_score: 0.415627 + 0.000897161\n",
      "[130]\tcv_agg's r2_score: 0.415627 + 0.00089622\n",
      "[131]\tcv_agg's r2_score: 0.415625 + 0.000896697\n"
     ]
    }
   ],
   "source": [
    "lgb_param_mae = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1,\n",
    "    'objective': 'mae',\n",
    "    'metric': 'none',\n",
    "    'tree_learner': 'data',\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "cvmodel_mae = lgb.cv(\n",
    "    lgb_param_mae,\n",
    "    lgbtrain_trans,\n",
    "    num_boost_round=10000,\n",
    "    folds=folds,\n",
    "    feval=metric_func_1,\n",
    "    metrics='none',\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=True)\n",
    "\n",
    "lgbmodel_mae = lgb.train(\n",
    "    lgb_param_mae,\n",
    "    lgbtrain_trans,\n",
    "    num_boost_round=len(cvmodel_mae['r2_score-mean']),\n",
    "    verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... as well as a RMSE model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's r2_score: -0.260506 + 0.00121804\n",
      "[2]\tcv_agg's r2_score: -0.223009 + 0.00111538\n",
      "[3]\tcv_agg's r2_score: -0.186053 + 0.001013\n",
      "[4]\tcv_agg's r2_score: -0.149712 + 0.000919921\n",
      "[5]\tcv_agg's r2_score: -0.114196 + 0.000803888\n",
      "[6]\tcv_agg's r2_score: -0.0799775 + 0.000763266\n",
      "[7]\tcv_agg's r2_score: -0.0471802 + 0.000649524\n",
      "[8]\tcv_agg's r2_score: -0.016189 + 0.000623505\n",
      "[9]\tcv_agg's r2_score: 0.012932 + 0.000539302\n",
      "[10]\tcv_agg's r2_score: 0.0401511 + 0.000450631\n",
      "[11]\tcv_agg's r2_score: 0.0654257 + 0.00040275\n",
      "[12]\tcv_agg's r2_score: 0.0886681 + 0.000453112\n",
      "[13]\tcv_agg's r2_score: 0.11001 + 0.000484833\n",
      "[14]\tcv_agg's r2_score: 0.129551 + 0.000534348\n",
      "[15]\tcv_agg's r2_score: 0.147344 + 0.000554739\n",
      "[16]\tcv_agg's r2_score: 0.163457 + 0.000615147\n",
      "[17]\tcv_agg's r2_score: 0.178106 + 0.000629504\n",
      "[18]\tcv_agg's r2_score: 0.191377 + 0.000658537\n",
      "[19]\tcv_agg's r2_score: 0.203336 + 0.000685421\n",
      "[20]\tcv_agg's r2_score: 0.214115 + 0.000731218\n",
      "[21]\tcv_agg's r2_score: 0.223823 + 0.000774406\n",
      "[22]\tcv_agg's r2_score: 0.232566 + 0.000790128\n",
      "[23]\tcv_agg's r2_score: 0.240404 + 0.000812776\n",
      "[24]\tcv_agg's r2_score: 0.247442 + 0.000841163\n",
      "[25]\tcv_agg's r2_score: 0.253789 + 0.000847956\n",
      "[26]\tcv_agg's r2_score: 0.259475 + 0.000860426\n",
      "[27]\tcv_agg's r2_score: 0.264575 + 0.000859053\n",
      "[28]\tcv_agg's r2_score: 0.269148 + 0.000866005\n",
      "[29]\tcv_agg's r2_score: 0.273251 + 0.000869603\n",
      "[30]\tcv_agg's r2_score: 0.276957 + 0.000888439\n",
      "[31]\tcv_agg's r2_score: 0.280269 + 0.000896891\n",
      "[32]\tcv_agg's r2_score: 0.283244 + 0.000907557\n",
      "[33]\tcv_agg's r2_score: 0.285911 + 0.000911886\n",
      "[34]\tcv_agg's r2_score: 0.288316 + 0.00092321\n",
      "[35]\tcv_agg's r2_score: 0.290476 + 0.000934341\n",
      "[36]\tcv_agg's r2_score: 0.292415 + 0.000949344\n",
      "[37]\tcv_agg's r2_score: 0.294156 + 0.000958948\n",
      "[38]\tcv_agg's r2_score: 0.295714 + 0.000967778\n",
      "[39]\tcv_agg's r2_score: 0.297122 + 0.000973472\n",
      "[40]\tcv_agg's r2_score: 0.298387 + 0.000983585\n",
      "[41]\tcv_agg's r2_score: 0.299525 + 0.000991971\n",
      "[42]\tcv_agg's r2_score: 0.300547 + 0.00100177\n",
      "[43]\tcv_agg's r2_score: 0.301467 + 0.00100863\n",
      "[44]\tcv_agg's r2_score: 0.302295 + 0.00101108\n",
      "[45]\tcv_agg's r2_score: 0.303038 + 0.00101614\n",
      "[46]\tcv_agg's r2_score: 0.303709 + 0.00101961\n",
      "[47]\tcv_agg's r2_score: 0.304311 + 0.00102182\n",
      "[48]\tcv_agg's r2_score: 0.304851 + 0.00102821\n",
      "[49]\tcv_agg's r2_score: 0.305338 + 0.00103058\n",
      "[50]\tcv_agg's r2_score: 0.305778 + 0.00103561\n",
      "[51]\tcv_agg's r2_score: 0.306173 + 0.00103508\n",
      "[52]\tcv_agg's r2_score: 0.306527 + 0.00104059\n",
      "[53]\tcv_agg's r2_score: 0.306844 + 0.00104615\n",
      "[54]\tcv_agg's r2_score: 0.307135 + 0.00104533\n",
      "[55]\tcv_agg's r2_score: 0.307395 + 0.00104583\n",
      "[56]\tcv_agg's r2_score: 0.307629 + 0.00104659\n",
      "[57]\tcv_agg's r2_score: 0.307839 + 0.00104607\n",
      "[58]\tcv_agg's r2_score: 0.308026 + 0.00105022\n",
      "[59]\tcv_agg's r2_score: 0.308197 + 0.00105269\n",
      "[60]\tcv_agg's r2_score: 0.308351 + 0.00105305\n",
      "[61]\tcv_agg's r2_score: 0.308486 + 0.00104939\n",
      "[62]\tcv_agg's r2_score: 0.308611 + 0.00105355\n",
      "[63]\tcv_agg's r2_score: 0.30872 + 0.00105786\n",
      "[64]\tcv_agg's r2_score: 0.308824 + 0.00105777\n",
      "[65]\tcv_agg's r2_score: 0.308917 + 0.00105928\n",
      "[66]\tcv_agg's r2_score: 0.308997 + 0.00105875\n",
      "[67]\tcv_agg's r2_score: 0.30907 + 0.00105944\n",
      "[68]\tcv_agg's r2_score: 0.309139 + 0.00106039\n",
      "[69]\tcv_agg's r2_score: 0.309199 + 0.00106125\n",
      "[70]\tcv_agg's r2_score: 0.309252 + 0.00106125\n",
      "[71]\tcv_agg's r2_score: 0.309302 + 0.0010616\n",
      "[72]\tcv_agg's r2_score: 0.309347 + 0.00106568\n",
      "[73]\tcv_agg's r2_score: 0.309386 + 0.00106572\n",
      "[74]\tcv_agg's r2_score: 0.309422 + 0.00106603\n",
      "[75]\tcv_agg's r2_score: 0.309453 + 0.00106713\n",
      "[76]\tcv_agg's r2_score: 0.309481 + 0.00106714\n",
      "[77]\tcv_agg's r2_score: 0.309507 + 0.00106876\n",
      "[78]\tcv_agg's r2_score: 0.309532 + 0.00107105\n",
      "[79]\tcv_agg's r2_score: 0.309553 + 0.00107437\n",
      "[80]\tcv_agg's r2_score: 0.309572 + 0.00107574\n",
      "[81]\tcv_agg's r2_score: 0.309589 + 0.00107652\n",
      "[82]\tcv_agg's r2_score: 0.309604 + 0.00107497\n",
      "[83]\tcv_agg's r2_score: 0.309616 + 0.00107581\n",
      "[84]\tcv_agg's r2_score: 0.309628 + 0.00107524\n",
      "[85]\tcv_agg's r2_score: 0.309638 + 0.00107295\n",
      "[86]\tcv_agg's r2_score: 0.309648 + 0.00107568\n",
      "[87]\tcv_agg's r2_score: 0.309656 + 0.00107941\n",
      "[88]\tcv_agg's r2_score: 0.309664 + 0.00107701\n",
      "[89]\tcv_agg's r2_score: 0.309671 + 0.00107915\n",
      "[90]\tcv_agg's r2_score: 0.309676 + 0.00107714\n",
      "[91]\tcv_agg's r2_score: 0.309682 + 0.00107763\n",
      "[92]\tcv_agg's r2_score: 0.309688 + 0.00107918\n",
      "[93]\tcv_agg's r2_score: 0.309692 + 0.00107831\n",
      "[94]\tcv_agg's r2_score: 0.309696 + 0.00107764\n",
      "[95]\tcv_agg's r2_score: 0.309698 + 0.00107828\n",
      "[96]\tcv_agg's r2_score: 0.309701 + 0.0010782\n",
      "[97]\tcv_agg's r2_score: 0.309704 + 0.00107832\n",
      "[98]\tcv_agg's r2_score: 0.309707 + 0.00107786\n",
      "[99]\tcv_agg's r2_score: 0.30971 + 0.00108007\n",
      "[100]\tcv_agg's r2_score: 0.309714 + 0.00108098\n",
      "[101]\tcv_agg's r2_score: 0.309715 + 0.00108152\n",
      "[102]\tcv_agg's r2_score: 0.309717 + 0.00108107\n",
      "[103]\tcv_agg's r2_score: 0.30972 + 0.00108073\n",
      "[104]\tcv_agg's r2_score: 0.309722 + 0.00108279\n",
      "[105]\tcv_agg's r2_score: 0.309725 + 0.00108204\n",
      "[106]\tcv_agg's r2_score: 0.309726 + 0.00108313\n",
      "[107]\tcv_agg's r2_score: 0.309729 + 0.00108265\n",
      "[108]\tcv_agg's r2_score: 0.309729 + 0.00108298\n",
      "[109]\tcv_agg's r2_score: 0.30973 + 0.00108333\n",
      "[110]\tcv_agg's r2_score: 0.309732 + 0.00108317\n",
      "[111]\tcv_agg's r2_score: 0.309735 + 0.00108321\n",
      "[112]\tcv_agg's r2_score: 0.309737 + 0.0010838\n",
      "[113]\tcv_agg's r2_score: 0.309737 + 0.00108408\n",
      "[114]\tcv_agg's r2_score: 0.309738 + 0.00108482\n",
      "[115]\tcv_agg's r2_score: 0.309739 + 0.00108533\n",
      "[116]\tcv_agg's r2_score: 0.309739 + 0.00108488\n",
      "[117]\tcv_agg's r2_score: 0.309741 + 0.00108432\n",
      "[118]\tcv_agg's r2_score: 0.309741 + 0.00108483\n",
      "[119]\tcv_agg's r2_score: 0.309742 + 0.00108502\n",
      "[120]\tcv_agg's r2_score: 0.309743 + 0.00108548\n",
      "[121]\tcv_agg's r2_score: 0.309744 + 0.00108467\n",
      "[122]\tcv_agg's r2_score: 0.309744 + 0.00108525\n",
      "[123]\tcv_agg's r2_score: 0.309746 + 0.00108483\n",
      "[124]\tcv_agg's r2_score: 0.309746 + 0.0010851\n",
      "[125]\tcv_agg's r2_score: 0.309746 + 0.00108588\n",
      "[126]\tcv_agg's r2_score: 0.309747 + 0.0010862\n",
      "[127]\tcv_agg's r2_score: 0.309747 + 0.0010863\n",
      "[128]\tcv_agg's r2_score: 0.309747 + 0.00108622\n",
      "[129]\tcv_agg's r2_score: 0.309748 + 0.00108609\n",
      "[130]\tcv_agg's r2_score: 0.30975 + 0.00108552\n",
      "[131]\tcv_agg's r2_score: 0.30975 + 0.0010858\n",
      "[132]\tcv_agg's r2_score: 0.309749 + 0.00108579\n",
      "[133]\tcv_agg's r2_score: 0.30975 + 0.00108692\n",
      "[134]\tcv_agg's r2_score: 0.30975 + 0.00108671\n",
      "[135]\tcv_agg's r2_score: 0.309751 + 0.00108702\n",
      "[136]\tcv_agg's r2_score: 0.309751 + 0.00108745\n",
      "[137]\tcv_agg's r2_score: 0.309751 + 0.00108747\n",
      "[138]\tcv_agg's r2_score: 0.309752 + 0.00108746\n",
      "[139]\tcv_agg's r2_score: 0.309752 + 0.00108716\n",
      "[140]\tcv_agg's r2_score: 0.309752 + 0.00108731\n",
      "[141]\tcv_agg's r2_score: 0.309752 + 0.00108713\n",
      "[142]\tcv_agg's r2_score: 0.309753 + 0.00108693\n",
      "[143]\tcv_agg's r2_score: 0.309753 + 0.00108673\n",
      "[144]\tcv_agg's r2_score: 0.309753 + 0.00108697\n",
      "[145]\tcv_agg's r2_score: 0.309753 + 0.00108698\n",
      "[146]\tcv_agg's r2_score: 0.309753 + 0.00108771\n",
      "[147]\tcv_agg's r2_score: 0.309753 + 0.00108798\n",
      "[148]\tcv_agg's r2_score: 0.309753 + 0.00108785\n",
      "[149]\tcv_agg's r2_score: 0.309753 + 0.0010878\n",
      "[150]\tcv_agg's r2_score: 0.309754 + 0.00108789\n",
      "[151]\tcv_agg's r2_score: 0.309754 + 0.00108851\n",
      "[152]\tcv_agg's r2_score: 0.309754 + 0.00108818\n",
      "[153]\tcv_agg's r2_score: 0.309754 + 0.00108866\n",
      "[154]\tcv_agg's r2_score: 0.309754 + 0.00108909\n",
      "[155]\tcv_agg's r2_score: 0.309755 + 0.00108889\n",
      "[156]\tcv_agg's r2_score: 0.309754 + 0.00108912\n",
      "[157]\tcv_agg's r2_score: 0.309754 + 0.00108917\n",
      "[158]\tcv_agg's r2_score: 0.309755 + 0.00108912\n",
      "[159]\tcv_agg's r2_score: 0.309755 + 0.00108902\n",
      "[160]\tcv_agg's r2_score: 0.309755 + 0.00108928\n",
      "[161]\tcv_agg's r2_score: 0.309755 + 0.00108882\n",
      "[162]\tcv_agg's r2_score: 0.309755 + 0.00108886\n",
      "[163]\tcv_agg's r2_score: 0.309755 + 0.00108901\n",
      "[164]\tcv_agg's r2_score: 0.309755 + 0.00108918\n",
      "[165]\tcv_agg's r2_score: 0.309755 + 0.00108931\n",
      "[166]\tcv_agg's r2_score: 0.309755 + 0.0010893\n",
      "[167]\tcv_agg's r2_score: 0.309755 + 0.00108937\n",
      "[168]\tcv_agg's r2_score: 0.309755 + 0.00108968\n",
      "[169]\tcv_agg's r2_score: 0.309755 + 0.00108969\n",
      "[170]\tcv_agg's r2_score: 0.309755 + 0.00108974\n",
      "[171]\tcv_agg's r2_score: 0.309755 + 0.0010898\n",
      "[172]\tcv_agg's r2_score: 0.309755 + 0.00108985\n",
      "[173]\tcv_agg's r2_score: 0.309755 + 0.00108977\n",
      "[174]\tcv_agg's r2_score: 0.309755 + 0.00109009\n",
      "[175]\tcv_agg's r2_score: 0.309755 + 0.00109017\n",
      "[176]\tcv_agg's r2_score: 0.309755 + 0.00108992\n",
      "[177]\tcv_agg's r2_score: 0.309755 + 0.00108983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178]\tcv_agg's r2_score: 0.309755 + 0.00108985\n",
      "[179]\tcv_agg's r2_score: 0.309755 + 0.00108995\n",
      "[180]\tcv_agg's r2_score: 0.309755 + 0.00108996\n",
      "[181]\tcv_agg's r2_score: 0.309755 + 0.00108991\n"
     ]
    }
   ],
   "source": [
    "lgb_param_rmse = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1,\n",
    "    'objective': 'rmse',\n",
    "    'metric': 'none',\n",
    "    'tree_learner': 'data',\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "cvmodel_rmse = lgb.cv(\n",
    "    lgb_param_rmse,\n",
    "    lgbtrain_trans,\n",
    "    num_boost_round=10000,\n",
    "    folds=folds,\n",
    "    feval=metric_func_1,\n",
    "    metrics='none',\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=True)\n",
    "\n",
    "lgbmodel_rmse = lgb.train(\n",
    "    lgb_param_rmse,\n",
    "    lgbtrain_trans,\n",
    "    num_boost_round=len(cvmodel_rmse['r2_score-mean']),\n",
    "    verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the predictions of all 3 models on the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_vec_test = lgbmodel0.predict(X_test.drop('y', axis=1))\n",
    "scores_ghl_test = lgbmodel.predict(X_test.drop('y', axis=1))\n",
    "scores_ghl_test = log_link_back_trans(scores_ghl_test + start_vec_test)\n",
    "\n",
    "scores_ghl_train = lgbmodel.predict(X_train.drop('y', axis=1))\n",
    "scores_ghl_train = log_link_back_trans(scores_ghl_train + start_vec)\n",
    "\n",
    "scores_rmse_train = lgbmodel_rmse.predict(X_train.drop('y', axis=1))\n",
    "scores_rmse_train = log_link_back_trans(scores_rmse_train)\n",
    "\n",
    "scores_rmse_test = lgbmodel_rmse.predict(X_test.drop('y', axis=1))\n",
    "scores_rmse_test = log_link_back_trans(scores_rmse_test)\n",
    "\n",
    "scores_mae_train = lgbmodel_mae.predict(X_train.drop('y', axis=1))\n",
    "scores_mae_train = log_link_back_trans(scores_mae_train)\n",
    "\n",
    "scores_mae_test = lgbmodel_mae.predict(X_test.drop('y', axis=1))\n",
    "scores_mae_test = log_link_back_trans(scores_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the predictions at hand we finally compute the $r^{2}$ score and the realtive error of the global mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error train ghl =  0.0769479300871\n",
      "mean error test ghl  =  0.0750770768387\n",
      "mean error train mae =  0.153917810958\n",
      "mean error test mae =  0.152216327981\n",
      "mean error train rmse =  0.361297656997\n",
      "mean error test rmse =  0.359984995732\n",
      "             ..........          \n",
      "r2 score train ghl =  0.433539836501\n",
      "r2 scores test ghl  =  0.430126436051\n",
      "r2 score train mae =  0.416110276056\n",
      "r2 score test mae =  0.413121524319\n",
      "r2 score train rmse =  0.310169634178\n",
      "r2 score test rmse =  0.307997141499\n"
     ]
    }
   ],
   "source": [
    "print(\"mean error train ghl = \",\n",
    "      (y_train.mean() - scores_ghl_train.mean()) / (y_train.mean()))\n",
    "print(\"mean error test ghl  = \",\n",
    "      (y_test.mean() - scores_ghl_test.mean()) / (y_test.mean()))\n",
    "print(\"mean error train mae = \",\n",
    "      (y_train.mean() - scores_mae_train.mean()) / (y_train.mean()))\n",
    "print(\"mean error test mae = \",\n",
    "      (y_test.mean() - scores_mae_test.mean()) / (y_test.mean()))\n",
    "print(\"mean error train rmse = \",\n",
    "      (y_train.mean() - scores_rmse_train.mean()) / (y_train.mean()))\n",
    "print(\"mean error test rmse = \",\n",
    "      (y_test.mean() - scores_rmse_test.mean()) / (y_test.mean()))\n",
    "print(\"             ..........          \")\n",
    "print(\"r2 score train ghl = \", r2_score(y_train, scores_ghl_train))\n",
    "print(\"r2 scores test ghl  = \", r2_score(y_test, scores_ghl_test))\n",
    "print(\"r2 score train mae = \", r2_score(y_train, scores_mae_train))\n",
    "print(\"r2 score test mae = \", r2_score(y_test, scores_mae_test))\n",
    "print(\"r2 score train rmse = \", r2_score(y_train, scores_rmse_train))\n",
    "print(\"r2 score test rmse = \", r2_score(y_test, scores_rmse_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
