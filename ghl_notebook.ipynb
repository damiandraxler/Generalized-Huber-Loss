{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Huber Regression\n",
    "This notebook illustrates how to use LightGBM to optimize the Generalized Huber loss (GHL) function with a log link function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import skewnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we consider a one dimensional linear problem of N data points with a skew normal error distribution and non-constant variance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50000\n",
    "sigma0 = 1\n",
    "xvec = np.linspace(-500, 500, N)\n",
    "yvec = np.array([\n",
    "    x + skewnorm.rvs(a=100, loc=0, scale=(sigma0 + np.abs(x)**(2)))\n",
    "    for x in xvec\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the log link function and it's inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_link_trans(x):\n",
    "    bool1 = x < 0\n",
    "    bool2 = x >= 0\n",
    "    x_trans = np.zeros([1, len(x)]).flatten()\n",
    "    x_trans[bool1] = -np.log1p(-x[bool1])\n",
    "    x_trans[bool2] = np.log1p(x[bool2])\n",
    "    return x_trans\n",
    "\n",
    "\n",
    "def log_link_back_trans(x):\n",
    "    bool1 = x < 0\n",
    "    bool2 = x >= 0\n",
    "    x_back_trans = np.zeros([1, len(x)]).flatten()\n",
    "    x_back_trans[bool1] = (1 - np.exp(-x[bool1]))\n",
    "    x_back_trans[bool2] = (np.exp(x[bool2]) - 1)\n",
    "    return x_back_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define 2 metric functions which both compute the $r^{2}$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric_func_1(yhat, dtrain):\n",
    "    y = dtrain.get_label().flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    #####################################################\n",
    "    yhat_back_trans = log_link_back_trans(yhat)\n",
    "    #####################################################\n",
    "    y_back_trans = log_link_back_trans(y)\n",
    "    #####################################################\n",
    "    val = r2_score(y_back_trans, yhat_back_trans)\n",
    "    return ('r2_score', val, True)\n",
    "\n",
    "\n",
    "def metric_func_2(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    yhat = yhat.flatten()\n",
    "    #####################################################\n",
    "    yhat_back_trans = log_link_back_trans(yhat)\n",
    "    #####################################################\n",
    "    val = r2_score(y, yhat_back_trans)\n",
    "    return ('r2_score', val, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines the objective function for LightGBM by computing the 1st and 2nd derivative of the GHL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generalized_huber_obj(yhat, dtrain, alpha=0.75):\n",
    "    y = dtrain.get_label()\n",
    "    yhat = yhat.flatten()\n",
    "\n",
    "    def sgn(x):\n",
    "        sig = np.sign(x)\n",
    "        sig[sig == 0] = 1\n",
    "        return sig\n",
    "\n",
    "    g = lambda x: sgn(x) * np.log(1 + np.abs(x))\n",
    "    ginv = lambda x: sgn(x) * (np.exp(np.abs(x)) - 1)\n",
    "    ginvp = lambda x: np.exp(np.abs(x))\n",
    "    ginvpp = lambda x: sgn(x) * np.exp(np.abs(x))\n",
    "\n",
    "    diff = g(y) - yhat\n",
    "    absdiff = np.abs(diff)\n",
    "\n",
    "    bool1_l = ((absdiff <= alpha) & (diff < 0))\n",
    "    bool1_r = ((absdiff <= alpha) & (diff >= 0))\n",
    "    bool2_l = ((absdiff > alpha) & (diff < 0))\n",
    "    bool2_r = ((absdiff > alpha) & (diff >= 0))\n",
    "\n",
    "    grad = np.zeros([1, len(yhat)]).flatten()\n",
    "    hess = np.zeros([1, len(yhat)]).flatten()\n",
    "\n",
    "    A = np.zeros([1, len(yhat)]).flatten()\n",
    "    Ap = np.zeros([1, len(yhat)]).flatten()\n",
    "    App = np.zeros([1, len(yhat)]).flatten()\n",
    "    B = np.zeros([1, len(yhat)]).flatten()\n",
    "\n",
    "    A[bool1_l] = ginv(yhat[bool1_l] - alpha) - ginv(yhat[bool1_l])\n",
    "    A[bool1_r] = ginv(yhat[bool1_r] + alpha) - ginv(yhat[bool1_r])\n",
    "    Ap[bool1_l] = ginvp(yhat[bool1_l] - alpha) - ginvp(yhat[bool1_l])\n",
    "    Ap[bool1_r] = ginvp(yhat[bool1_r] + alpha) - ginvp(yhat[bool1_r])\n",
    "    App[bool1_l] = ginvpp(yhat[bool1_l] - alpha) - ginvpp(yhat[bool1_l])\n",
    "    App[bool1_r] = ginvpp(yhat[bool1_r] + alpha) - ginvpp(yhat[bool1_r])\n",
    "\n",
    "    A[bool2_l] = ginv(yhat[bool2_l] - alpha) - ginv(yhat[bool2_l])\n",
    "    A[bool2_r] = ginv(yhat[bool2_r] + alpha) - ginv(yhat[bool2_r])\n",
    "    Ap[bool2_l] = ginvp(yhat[bool2_l] - alpha) - ginvp(yhat[bool2_l])\n",
    "    Ap[bool2_r] = ginvp(yhat[bool2_r] + alpha) - ginvp(yhat[bool2_r])\n",
    "    App[bool2_l] = ginvpp(yhat[bool2_l] - alpha) - ginvpp(yhat[bool2_l])\n",
    "    App[bool2_r] = ginvpp(yhat[bool2_r] + alpha) - ginvpp(yhat[bool2_r])\n",
    "\n",
    "    B[bool1_l] = y[bool1_l] - ginv(g(y[bool1_l]) + alpha)\n",
    "    B[bool1_r] = y[bool1_r] - ginv(g(y[bool1_r]) - alpha)\n",
    "\n",
    "    grad[bool1_l] = -2*(y[bool1_l]-ginv(yhat[bool1_l]))*ginvp(yhat[bool1_l])*(1/np.abs(A[bool1_l]) + 1/np.abs(B[bool1_l])) \\\n",
    "                    -(y[bool1_l]-ginv(yhat[bool1_l]))**2*(1/(np.abs(A[bool1_l])**2))*sgn(A[bool1_l])*Ap[bool1_l]\n",
    "\n",
    "    grad[bool1_r] = -2*(y[bool1_r]-ginv(yhat[bool1_r]))*ginvp(yhat[bool1_r])*(1/np.abs(A[bool1_r]) + 1/np.abs(B[bool1_r])) \\\n",
    "                    -(y[bool1_r]-ginv(yhat[bool1_r]))**2*(1/(np.abs(A[bool1_r])**2))*sgn(A[bool1_r])*Ap[bool1_r]\n",
    "\n",
    "    hess[bool1_l] = 2*(ginvp(yhat[bool1_l])**2 - (y[bool1_l]-ginv(yhat[bool1_l]))*ginvpp(yhat[bool1_l]))*(1/np.abs(A[bool1_l]) + 1/np.abs(B[bool1_l])) \\\n",
    "                    +4*(y[bool1_l]-ginv(yhat[bool1_l]))*ginvp(yhat[bool1_l])*(1/(np.abs(A[bool1_l])**2))*sgn(A[bool1_l])*Ap[bool1_l] \\\n",
    "                    +2*(y[bool1_l]-ginv(yhat[bool1_l]))**2*(1/(np.abs(A[bool1_l])**3))*Ap[bool1_l]**2 \\\n",
    "                    -(y[bool1_l]-ginv(yhat[bool1_l]))**2*(1/(np.abs(A[bool1_l])**2))*sgn(A[bool1_l])*App[bool1_l]\n",
    "\n",
    "    hess[bool1_r] = 2*(ginvp(yhat[bool1_r])**2 - (y[bool1_r]-ginv(yhat[bool1_r]))*ginvpp(yhat[bool1_r]))*(1/np.abs(A[bool1_r]) + 1/np.abs(B[bool1_r])) \\\n",
    "                   +4*(y[bool1_r]-ginv(yhat[bool1_r]))*ginvp(yhat[bool1_r])*(1/(np.abs(A[bool1_r])**2))*sgn(A[bool1_r])*Ap[bool1_r] \\\n",
    "                   +2*(y[bool1_r]-ginv(yhat[bool1_r]))**2*(1/(np.abs(A[bool1_r])**3))*Ap[bool1_r]**2 \\\n",
    "                    -(y[bool1_r]-ginv(yhat[bool1_r]))**2*(1/(np.abs(A[bool1_r])**2))*sgn(A[bool1_r])*App[bool1_r]\n",
    "\n",
    "    grad[bool2_l] = -4 * sgn(y[bool2_l] - ginv(yhat[bool2_l])) * ginvp(\n",
    "        yhat[bool2_l]) - sgn(A[bool2_l]) * Ap[bool2_l]\n",
    "\n",
    "    grad[bool2_r] = -4 * sgn(y[bool2_r] - ginv(yhat[bool2_r])) * ginvp(\n",
    "        yhat[bool2_r]) - sgn(A[bool2_r]) * Ap[bool2_r]\n",
    "\n",
    "    hess[bool2_l] = -4 * sgn(y[bool2_l] - ginv(yhat[bool2_l])) * ginvpp(\n",
    "        yhat[bool2_l]) - sgn(A[bool2_l]) * App[bool2_l]\n",
    "\n",
    "    hess[bool2_r] = -4 * sgn(y[bool2_r] - ginv(yhat[bool2_r])) * ginvpp(\n",
    "        yhat[bool2_r]) - sgn(A[bool2_r]) * App[bool2_r]\n",
    "\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we put our data into a Dataframe, define the train and test sets and compute 3 cv folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.column_stack((xvec, yvec)), columns=['x', 'y'])\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.3)\n",
    "y_train = X_train['y']\n",
    "y_test = X_test['y']\n",
    "\n",
    "groups = X_train.index\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "group_kfold.get_n_splits(X_train, groups)\n",
    "folds = list(group_kfold.split(X_train, y_train, groups=groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by training a shallow Lightgbm model whose output will be the starting vector of the GHL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_param_start_vector = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1,\n",
    "    'alpha': 2,\n",
    "    'objective': 'huber',\n",
    "    'metric': 'none',\n",
    "    'tree_learner': 'data',\n",
    "    'verbosity': 3\n",
    "}\n",
    "\n",
    "lgbtrain_trans = lgb.Dataset(\n",
    "    X_train.drop('y', axis=1), label=log_link_trans(y_train))\n",
    "lgbmodel0 = lgb.train(\n",
    "    lgb_param_start_vector, lgbtrain_trans, num_boost_round=20)\n",
    "\n",
    "start_vec = lgbmodel0.predict(X_train.drop('y', axis=1))\n",
    "lgbtrain = lgb.Dataset(\n",
    "    X_train.drop('y', axis=1), init_score=start_vec, label=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform a hyperparameter search to find the optimal $\\alpha$ parameter in the GHL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.147368421053\n",
      "0.194736842105\n",
      "0.242105263158\n",
      "0.289473684211\n",
      "0.336842105263\n",
      "0.384210526316\n",
      "0.431578947368\n",
      "0.478947368421\n",
      "0.526315789474\n",
      "0.573684210526\n",
      "0.621052631579\n",
      "0.668421052632\n",
      "0.715789473684\n",
      "0.763157894737\n",
      "0.810526315789\n",
      "0.857894736842\n",
      "0.905263157895\n",
      "0.952631578947\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "lgb_param_ghl = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1,\n",
    "    'metric': 'none',\n",
    "    'max_delta_step': 3,\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "kwargsvec = np.linspace(0.1, 1, 20)\n",
    "chvec = np.zeros(20)\n",
    "for k in range(20):\n",
    "    kwargs = {'alpha': kwargsvec[k]}\n",
    "    cvmodel = lgb.cv(\n",
    "        lgb_param_ghl,\n",
    "        lgbtrain,\n",
    "        num_boost_round=10000,\n",
    "        folds=folds,\n",
    "        fobj=(lambda a, b: generalized_huber_obj(a, b, **kwargs)),\n",
    "        metrics='none',\n",
    "        feval=metric_func_2,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=False)\n",
    "    chvec[k] = cvmodel['r2_score-mean'][-1]\n",
    "    print(kwargsvec[k])\n",
    "\n",
    "best_iter = np.argmax(chvec)\n",
    "alpha_ch = kwargsvec[best_iter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having found the optimal $\\alpha$ we can now train the GHL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's r2_score: 0.315434 + 0.00452089\n",
      "[2]\tcv_agg's r2_score: 0.342636 + 0.00441996\n",
      "[3]\tcv_agg's r2_score: 0.361075 + 0.00422776\n",
      "[4]\tcv_agg's r2_score: 0.374538 + 0.00399017\n",
      "[5]\tcv_agg's r2_score: 0.384616 + 0.00376263\n",
      "[6]\tcv_agg's r2_score: 0.392602 + 0.00359278\n",
      "[7]\tcv_agg's r2_score: 0.398893 + 0.00343957\n",
      "[8]\tcv_agg's r2_score: 0.404034 + 0.0034064\n",
      "[9]\tcv_agg's r2_score: 0.408239 + 0.00334803\n",
      "[10]\tcv_agg's r2_score: 0.411665 + 0.00334058\n",
      "[11]\tcv_agg's r2_score: 0.414488 + 0.00332364\n",
      "[12]\tcv_agg's r2_score: 0.416813 + 0.00332702\n",
      "[13]\tcv_agg's r2_score: 0.418806 + 0.00338979\n",
      "[14]\tcv_agg's r2_score: 0.420461 + 0.00341538\n",
      "[15]\tcv_agg's r2_score: 0.42186 + 0.00344469\n",
      "[16]\tcv_agg's r2_score: 0.423037 + 0.00345049\n",
      "[17]\tcv_agg's r2_score: 0.424073 + 0.00352018\n",
      "[18]\tcv_agg's r2_score: 0.424928 + 0.00356757\n",
      "[19]\tcv_agg's r2_score: 0.425669 + 0.00359542\n",
      "[20]\tcv_agg's r2_score: 0.426287 + 0.003615\n",
      "[21]\tcv_agg's r2_score: 0.426843 + 0.00366473\n",
      "[22]\tcv_agg's r2_score: 0.42726 + 0.00368291\n",
      "[23]\tcv_agg's r2_score: 0.427673 + 0.00372267\n",
      "[24]\tcv_agg's r2_score: 0.427992 + 0.00373135\n",
      "[25]\tcv_agg's r2_score: 0.428291 + 0.00374634\n",
      "[26]\tcv_agg's r2_score: 0.428536 + 0.00374438\n",
      "[27]\tcv_agg's r2_score: 0.428774 + 0.00376685\n",
      "[28]\tcv_agg's r2_score: 0.428965 + 0.00376234\n",
      "[29]\tcv_agg's r2_score: 0.429141 + 0.0037505\n",
      "[30]\tcv_agg's r2_score: 0.429296 + 0.00375736\n",
      "[31]\tcv_agg's r2_score: 0.429416 + 0.00376168\n",
      "[32]\tcv_agg's r2_score: 0.429531 + 0.0037691\n",
      "[33]\tcv_agg's r2_score: 0.429613 + 0.00375064\n",
      "[34]\tcv_agg's r2_score: 0.4297 + 0.00377523\n",
      "[35]\tcv_agg's r2_score: 0.429793 + 0.00378926\n",
      "[36]\tcv_agg's r2_score: 0.429849 + 0.0037936\n",
      "[37]\tcv_agg's r2_score: 0.429898 + 0.00378826\n",
      "[38]\tcv_agg's r2_score: 0.429939 + 0.00378994\n",
      "[39]\tcv_agg's r2_score: 0.429982 + 0.00378153\n",
      "[40]\tcv_agg's r2_score: 0.430006 + 0.00378584\n",
      "[41]\tcv_agg's r2_score: 0.430038 + 0.00377812\n",
      "[42]\tcv_agg's r2_score: 0.430061 + 0.00377419\n",
      "[43]\tcv_agg's r2_score: 0.430085 + 0.00376664\n",
      "[44]\tcv_agg's r2_score: 0.430094 + 0.00376766\n",
      "[45]\tcv_agg's r2_score: 0.430106 + 0.00375643\n",
      "[46]\tcv_agg's r2_score: 0.430114 + 0.00375929\n",
      "[47]\tcv_agg's r2_score: 0.430124 + 0.00375422\n",
      "[48]\tcv_agg's r2_score: 0.43012 + 0.00373238\n",
      "[49]\tcv_agg's r2_score: 0.430129 + 0.00373034\n",
      "[50]\tcv_agg's r2_score: 0.430124 + 0.00373079\n",
      "[51]\tcv_agg's r2_score: 0.430123 + 0.0037304\n",
      "[52]\tcv_agg's r2_score: 0.430123 + 0.00372821\n",
      "[53]\tcv_agg's r2_score: 0.430111 + 0.00371957\n",
      "[54]\tcv_agg's r2_score: 0.430107 + 0.0037152\n",
      "[55]\tcv_agg's r2_score: 0.430109 + 0.00371883\n",
      "[56]\tcv_agg's r2_score: 0.430099 + 0.00372159\n",
      "[57]\tcv_agg's r2_score: 0.430092 + 0.00371372\n",
      "[58]\tcv_agg's r2_score: 0.430087 + 0.00372147\n",
      "[59]\tcv_agg's r2_score: 0.430069 + 0.00371713\n",
      "[60]\tcv_agg's r2_score: 0.430057 + 0.00372958\n",
      "[61]\tcv_agg's r2_score: 0.430048 + 0.00372874\n",
      "[62]\tcv_agg's r2_score: 0.430043 + 0.00373409\n",
      "[63]\tcv_agg's r2_score: 0.430031 + 0.0037251\n",
      "[64]\tcv_agg's r2_score: 0.43002 + 0.00372609\n",
      "[65]\tcv_agg's r2_score: 0.430017 + 0.00373064\n",
      "[66]\tcv_agg's r2_score: 0.43 + 0.00372723\n",
      "[67]\tcv_agg's r2_score: 0.429984 + 0.00372566\n",
      "[68]\tcv_agg's r2_score: 0.429964 + 0.00371986\n",
      "[69]\tcv_agg's r2_score: 0.429955 + 0.00372791\n",
      "crazy huber best alpha:  0.763157894737\n",
      "crazy huber best iteration:  49\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'alpha': alpha_ch}\n",
    "cvmodel = lgb.cv(\n",
    "    lgb_param_ghl,\n",
    "    lgbtrain,\n",
    "    num_boost_round=10000,\n",
    "    folds=folds,\n",
    "    fobj=(lambda a, b: generalized_huber_obj(a, b, **kwargs)),\n",
    "    metrics='none',\n",
    "    feval=metric_func_2,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=True)\n",
    "\n",
    "print('generalized huber best alpha: ', alpha_ch)\n",
    "print('generalized huber best iteration: ', len(cvmodel['r2_score-mean']))\n",
    "\n",
    "lgbmodel = lgb.train(\n",
    "    lgb_param_ghl,\n",
    "    lgbtrain,\n",
    "    num_boost_round=len(cvmodel['r2_score-mean']),\n",
    "    fobj=(lambda a, b: generalized_huber_obj(a, b, **kwargs)),\n",
    "    feval=metric_func_2,\n",
    "    verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next a MAE model is trained on $\\log(y)$ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's r2_score: -0.111879 + 0.00170896\n",
      "[2]\tcv_agg's r2_score: -0.0633219 + 0.0020626\n",
      "[3]\tcv_agg's r2_score: -0.0184442 + 0.00261469\n",
      "[4]\tcv_agg's r2_score: 0.023344 + 0.00322538\n",
      "[5]\tcv_agg's r2_score: 0.0622578 + 0.00392642\n",
      "[6]\tcv_agg's r2_score: 0.0987785 + 0.00430932\n",
      "[7]\tcv_agg's r2_score: 0.132495 + 0.00469293\n",
      "[8]\tcv_agg's r2_score: 0.163179 + 0.00506662\n",
      "[9]\tcv_agg's r2_score: 0.191234 + 0.00532241\n",
      "[10]\tcv_agg's r2_score: 0.216405 + 0.00559689\n",
      "[11]\tcv_agg's r2_score: 0.238911 + 0.00587978\n",
      "[12]\tcv_agg's r2_score: 0.259147 + 0.00620783\n",
      "[13]\tcv_agg's r2_score: 0.277605 + 0.00608483\n",
      "[14]\tcv_agg's r2_score: 0.293773 + 0.00615044\n",
      "[15]\tcv_agg's r2_score: 0.308201 + 0.00597854\n",
      "[16]\tcv_agg's r2_score: 0.320864 + 0.00591604\n",
      "[17]\tcv_agg's r2_score: 0.331967 + 0.00574992\n",
      "[18]\tcv_agg's r2_score: 0.341958 + 0.00545441\n",
      "[19]\tcv_agg's r2_score: 0.350699 + 0.00520376\n",
      "[20]\tcv_agg's r2_score: 0.358451 + 0.00495837\n",
      "[21]\tcv_agg's r2_score: 0.365124 + 0.00473855\n",
      "[22]\tcv_agg's r2_score: 0.370948 + 0.00463676\n",
      "[23]\tcv_agg's r2_score: 0.375886 + 0.00456787\n",
      "[24]\tcv_agg's r2_score: 0.380536 + 0.00431353\n",
      "[25]\tcv_agg's r2_score: 0.384484 + 0.00434206\n",
      "[26]\tcv_agg's r2_score: 0.387911 + 0.00442087\n",
      "[27]\tcv_agg's r2_score: 0.391004 + 0.00431813\n",
      "[28]\tcv_agg's r2_score: 0.39366 + 0.00415804\n",
      "[29]\tcv_agg's r2_score: 0.395912 + 0.00400428\n",
      "[30]\tcv_agg's r2_score: 0.397946 + 0.00388544\n",
      "[31]\tcv_agg's r2_score: 0.399649 + 0.00379748\n",
      "[32]\tcv_agg's r2_score: 0.401142 + 0.00370123\n",
      "[33]\tcv_agg's r2_score: 0.402561 + 0.00384375\n",
      "[34]\tcv_agg's r2_score: 0.403759 + 0.00379963\n",
      "[35]\tcv_agg's r2_score: 0.404816 + 0.0036602\n",
      "[36]\tcv_agg's r2_score: 0.405821 + 0.00362335\n",
      "[37]\tcv_agg's r2_score: 0.406868 + 0.00364466\n",
      "[38]\tcv_agg's r2_score: 0.407633 + 0.00372533\n",
      "[39]\tcv_agg's r2_score: 0.408301 + 0.0037327\n",
      "[40]\tcv_agg's r2_score: 0.408918 + 0.00379924\n",
      "[41]\tcv_agg's r2_score: 0.409507 + 0.00382957\n",
      "[42]\tcv_agg's r2_score: 0.409976 + 0.00389399\n",
      "[43]\tcv_agg's r2_score: 0.410396 + 0.00390889\n",
      "[44]\tcv_agg's r2_score: 0.410799 + 0.00388331\n",
      "[45]\tcv_agg's r2_score: 0.411089 + 0.0038434\n",
      "[46]\tcv_agg's r2_score: 0.411334 + 0.00380122\n",
      "[47]\tcv_agg's r2_score: 0.411568 + 0.0038289\n",
      "[48]\tcv_agg's r2_score: 0.411863 + 0.0038801\n",
      "[49]\tcv_agg's r2_score: 0.412179 + 0.00382523\n",
      "[50]\tcv_agg's r2_score: 0.412399 + 0.00378424\n",
      "[51]\tcv_agg's r2_score: 0.412583 + 0.00380148\n",
      "[52]\tcv_agg's r2_score: 0.412717 + 0.00379965\n",
      "[53]\tcv_agg's r2_score: 0.412851 + 0.0037412\n",
      "[54]\tcv_agg's r2_score: 0.412922 + 0.00372865\n",
      "[55]\tcv_agg's r2_score: 0.413034 + 0.00367579\n",
      "[56]\tcv_agg's r2_score: 0.41309 + 0.003663\n",
      "[57]\tcv_agg's r2_score: 0.413229 + 0.00364511\n",
      "[58]\tcv_agg's r2_score: 0.413268 + 0.00359195\n",
      "[59]\tcv_agg's r2_score: 0.413297 + 0.00353117\n",
      "[60]\tcv_agg's r2_score: 0.41331 + 0.00352493\n",
      "[61]\tcv_agg's r2_score: 0.413356 + 0.00348625\n",
      "[62]\tcv_agg's r2_score: 0.413416 + 0.0034686\n",
      "[63]\tcv_agg's r2_score: 0.413425 + 0.00343846\n",
      "[64]\tcv_agg's r2_score: 0.413427 + 0.00340328\n",
      "[65]\tcv_agg's r2_score: 0.413439 + 0.00341459\n",
      "[66]\tcv_agg's r2_score: 0.413466 + 0.003409\n",
      "[67]\tcv_agg's r2_score: 0.413484 + 0.00342583\n",
      "[68]\tcv_agg's r2_score: 0.413478 + 0.00342962\n",
      "[69]\tcv_agg's r2_score: 0.413528 + 0.00342526\n",
      "[70]\tcv_agg's r2_score: 0.413519 + 0.00345265\n",
      "[71]\tcv_agg's r2_score: 0.413461 + 0.00344668\n",
      "[72]\tcv_agg's r2_score: 0.413439 + 0.00346134\n",
      "[73]\tcv_agg's r2_score: 0.413452 + 0.00340114\n",
      "[74]\tcv_agg's r2_score: 0.413485 + 0.00341169\n",
      "[75]\tcv_agg's r2_score: 0.413522 + 0.00343112\n",
      "[76]\tcv_agg's r2_score: 0.413522 + 0.00346657\n",
      "[77]\tcv_agg's r2_score: 0.413517 + 0.00345966\n",
      "[78]\tcv_agg's r2_score: 0.413522 + 0.00346852\n",
      "[79]\tcv_agg's r2_score: 0.413509 + 0.00350508\n",
      "[80]\tcv_agg's r2_score: 0.41351 + 0.00351116\n",
      "[81]\tcv_agg's r2_score: 0.413476 + 0.00348725\n",
      "[82]\tcv_agg's r2_score: 0.413487 + 0.00349702\n",
      "[83]\tcv_agg's r2_score: 0.41347 + 0.00348644\n",
      "[84]\tcv_agg's r2_score: 0.413473 + 0.00349059\n",
      "[85]\tcv_agg's r2_score: 0.413477 + 0.00349852\n",
      "[86]\tcv_agg's r2_score: 0.413469 + 0.00349893\n",
      "[87]\tcv_agg's r2_score: 0.413447 + 0.00348956\n",
      "[88]\tcv_agg's r2_score: 0.413432 + 0.00349342\n",
      "[89]\tcv_agg's r2_score: 0.413427 + 0.00349823\n"
     ]
    }
   ],
   "source": [
    "lgb_param_mae = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1,\n",
    "    'objective': 'mae',\n",
    "    'metric': 'none',\n",
    "    'tree_learner': 'data',\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "cvmodel_mae = lgb.cv(\n",
    "    lgb_param_mae,\n",
    "    lgbtrain_trans,\n",
    "    num_boost_round=10000,\n",
    "    folds=folds,\n",
    "    feval=metric_func_1,\n",
    "    metrics='none',\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=True)\n",
    "\n",
    "lgbmodel_mae = lgb.train(\n",
    "    lgb_param_mae,\n",
    "    lgbtrain_trans,\n",
    "    num_boost_round=len(cvmodel_mae['r2_score-mean']),\n",
    "    verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... as well as a RMSE model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's r2_score: -0.2598 + 0.00272392\n",
      "[2]\tcv_agg's r2_score: -0.221886 + 0.00162379\n",
      "[3]\tcv_agg's r2_score: -0.184382 + 0.00060316\n",
      "[4]\tcv_agg's r2_score: -0.147394 + 0.000372647\n",
      "[5]\tcv_agg's r2_score: -0.111402 + 0.00144645\n",
      "[6]\tcv_agg's r2_score: -0.0765694 + 0.00243073\n",
      "[7]\tcv_agg's r2_score: -0.0432549 + 0.00340817\n",
      "[8]\tcv_agg's r2_score: -0.0115667 + 0.00420541\n",
      "[9]\tcv_agg's r2_score: 0.0180644 + 0.00497593\n",
      "[10]\tcv_agg's r2_score: 0.0457333 + 0.0055736\n",
      "[11]\tcv_agg's r2_score: 0.0714348 + 0.00606819\n",
      "[12]\tcv_agg's r2_score: 0.0950789 + 0.00650422\n",
      "[13]\tcv_agg's r2_score: 0.11669 + 0.00693169\n",
      "[14]\tcv_agg's r2_score: 0.136469 + 0.00726533\n",
      "[15]\tcv_agg's r2_score: 0.154487 + 0.00751\n",
      "[16]\tcv_agg's r2_score: 0.170871 + 0.00766347\n",
      "[17]\tcv_agg's r2_score: 0.185628 + 0.00775919\n",
      "[18]\tcv_agg's r2_score: 0.198968 + 0.00782995\n",
      "[19]\tcv_agg's r2_score: 0.210918 + 0.00791676\n",
      "[20]\tcv_agg's r2_score: 0.221699 + 0.00794811\n",
      "[21]\tcv_agg's r2_score: 0.231464 + 0.00802169\n",
      "[22]\tcv_agg's r2_score: 0.240198 + 0.00806554\n",
      "[23]\tcv_agg's r2_score: 0.248015 + 0.00802202\n",
      "[24]\tcv_agg's r2_score: 0.254961 + 0.00810542\n",
      "[25]\tcv_agg's r2_score: 0.261188 + 0.0080907\n",
      "[26]\tcv_agg's r2_score: 0.266707 + 0.00804135\n",
      "[27]\tcv_agg's r2_score: 0.271801 + 0.008065\n",
      "[28]\tcv_agg's r2_score: 0.276284 + 0.00800879\n",
      "[29]\tcv_agg's r2_score: 0.280379 + 0.00791777\n",
      "[30]\tcv_agg's r2_score: 0.284034 + 0.00781296\n",
      "[31]\tcv_agg's r2_score: 0.287323 + 0.00782241\n",
      "[32]\tcv_agg's r2_score: 0.290227 + 0.00777568\n",
      "[33]\tcv_agg's r2_score: 0.292891 + 0.00771563\n",
      "[34]\tcv_agg's r2_score: 0.295186 + 0.0076295\n",
      "[35]\tcv_agg's r2_score: 0.297299 + 0.00752338\n",
      "[36]\tcv_agg's r2_score: 0.299194 + 0.00744803\n",
      "[37]\tcv_agg's r2_score: 0.300899 + 0.00745724\n",
      "[38]\tcv_agg's r2_score: 0.302469 + 0.00733532\n",
      "[39]\tcv_agg's r2_score: 0.303822 + 0.00731405\n",
      "[40]\tcv_agg's r2_score: 0.304984 + 0.00724611\n",
      "[41]\tcv_agg's r2_score: 0.306063 + 0.00721868\n",
      "[42]\tcv_agg's r2_score: 0.307078 + 0.00710127\n",
      "[43]\tcv_agg's r2_score: 0.307941 + 0.00706883\n",
      "[44]\tcv_agg's r2_score: 0.308723 + 0.00701829\n",
      "[45]\tcv_agg's r2_score: 0.309442 + 0.00703236\n",
      "[46]\tcv_agg's r2_score: 0.31005 + 0.00700793\n",
      "[47]\tcv_agg's r2_score: 0.310626 + 0.00695017\n",
      "[48]\tcv_agg's r2_score: 0.311131 + 0.00696595\n",
      "[49]\tcv_agg's r2_score: 0.311559 + 0.00699275\n",
      "[50]\tcv_agg's r2_score: 0.311975 + 0.00698176\n",
      "[51]\tcv_agg's r2_score: 0.312319 + 0.00695133\n",
      "[52]\tcv_agg's r2_score: 0.312667 + 0.00695104\n",
      "[53]\tcv_agg's r2_score: 0.313021 + 0.00693194\n",
      "[54]\tcv_agg's r2_score: 0.313301 + 0.00691183\n",
      "[55]\tcv_agg's r2_score: 0.313581 + 0.00690812\n",
      "[56]\tcv_agg's r2_score: 0.313834 + 0.00689667\n",
      "[57]\tcv_agg's r2_score: 0.314038 + 0.00688228\n",
      "[58]\tcv_agg's r2_score: 0.314176 + 0.0068192\n",
      "[59]\tcv_agg's r2_score: 0.314309 + 0.00678573\n",
      "[60]\tcv_agg's r2_score: 0.314465 + 0.00679042\n",
      "[61]\tcv_agg's r2_score: 0.314572 + 0.00672357\n",
      "[62]\tcv_agg's r2_score: 0.314704 + 0.00669096\n",
      "[63]\tcv_agg's r2_score: 0.314725 + 0.00668289\n",
      "[64]\tcv_agg's r2_score: 0.314807 + 0.00665278\n",
      "[65]\tcv_agg's r2_score: 0.314878 + 0.00656973\n",
      "[66]\tcv_agg's r2_score: 0.314937 + 0.00654662\n",
      "[67]\tcv_agg's r2_score: 0.31494 + 0.00654121\n",
      "[68]\tcv_agg's r2_score: 0.314952 + 0.00652215\n",
      "[69]\tcv_agg's r2_score: 0.315003 + 0.00651885\n",
      "[70]\tcv_agg's r2_score: 0.315024 + 0.00650942\n",
      "[71]\tcv_agg's r2_score: 0.315014 + 0.00648595\n",
      "[72]\tcv_agg's r2_score: 0.315064 + 0.00646701\n",
      "[73]\tcv_agg's r2_score: 0.315067 + 0.00646683\n",
      "[74]\tcv_agg's r2_score: 0.315073 + 0.00644957\n",
      "[75]\tcv_agg's r2_score: 0.315107 + 0.00646106\n",
      "[76]\tcv_agg's r2_score: 0.315096 + 0.00644444\n",
      "[77]\tcv_agg's r2_score: 0.315116 + 0.00646311\n",
      "[78]\tcv_agg's r2_score: 0.315123 + 0.00644734\n",
      "[79]\tcv_agg's r2_score: 0.315121 + 0.00643317\n",
      "[80]\tcv_agg's r2_score: 0.315134 + 0.00643549\n",
      "[81]\tcv_agg's r2_score: 0.315132 + 0.00642808\n",
      "[82]\tcv_agg's r2_score: 0.315116 + 0.00640964\n",
      "[83]\tcv_agg's r2_score: 0.315139 + 0.00642361\n",
      "[84]\tcv_agg's r2_score: 0.315149 + 0.00639547\n",
      "[85]\tcv_agg's r2_score: 0.315148 + 0.00639096\n",
      "[86]\tcv_agg's r2_score: 0.315141 + 0.0064106\n",
      "[87]\tcv_agg's r2_score: 0.315155 + 0.00640976\n",
      "[88]\tcv_agg's r2_score: 0.31515 + 0.00640495\n",
      "[89]\tcv_agg's r2_score: 0.315142 + 0.00639559\n",
      "[90]\tcv_agg's r2_score: 0.315139 + 0.0063904\n",
      "[91]\tcv_agg's r2_score: 0.315132 + 0.0063968\n",
      "[92]\tcv_agg's r2_score: 0.315124 + 0.00639558\n",
      "[93]\tcv_agg's r2_score: 0.315142 + 0.00638239\n",
      "[94]\tcv_agg's r2_score: 0.315128 + 0.0063791\n",
      "[95]\tcv_agg's r2_score: 0.315125 + 0.00639559\n",
      "[96]\tcv_agg's r2_score: 0.315121 + 0.00639337\n",
      "[97]\tcv_agg's r2_score: 0.315117 + 0.00638198\n",
      "[98]\tcv_agg's r2_score: 0.315124 + 0.00638962\n",
      "[99]\tcv_agg's r2_score: 0.31511 + 0.00638933\n",
      "[100]\tcv_agg's r2_score: 0.315109 + 0.00638634\n",
      "[101]\tcv_agg's r2_score: 0.315126 + 0.00638982\n",
      "[102]\tcv_agg's r2_score: 0.315118 + 0.00638297\n",
      "[103]\tcv_agg's r2_score: 0.315118 + 0.00639372\n",
      "[104]\tcv_agg's r2_score: 0.315107 + 0.00638904\n",
      "[105]\tcv_agg's r2_score: 0.315102 + 0.00636741\n",
      "[106]\tcv_agg's r2_score: 0.315095 + 0.00636619\n",
      "[107]\tcv_agg's r2_score: 0.315099 + 0.0063716\n"
     ]
    }
   ],
   "source": [
    "lgb_param_rmse = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1,\n",
    "    'objective': 'rmse',\n",
    "    'metric': 'none',\n",
    "    'tree_learner': 'data',\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "cvmodel_rmse = lgb.cv(\n",
    "    lgb_param_rmse,\n",
    "    lgbtrain_trans,\n",
    "    num_boost_round=10000,\n",
    "    folds=folds,\n",
    "    feval=metric_func_1,\n",
    "    metrics='none',\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=True)\n",
    "\n",
    "lgbmodel_rmse = lgb.train(\n",
    "    lgb_param_rmse,\n",
    "    lgbtrain_trans,\n",
    "    num_boost_round=len(cvmodel_rmse['r2_score-mean']),\n",
    "    verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the predictions of all 3 models on the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_vec_test = lgbmodel0.predict(X_test.drop('y', axis=1))\n",
    "scores_ghl_test = lgbmodel.predict(X_test.drop('y', axis=1))\n",
    "scores_ghl_test = log_link_back_trans(scores_ghl_test + start_vec_test)\n",
    "\n",
    "scores_ghl_train = lgbmodel.predict(X_train.drop('y', axis=1))\n",
    "scores_ghl_train = log_link_back_trans(scores_ghl_train + start_vec)\n",
    "\n",
    "scores_rmse_train = lgbmodel_rmse.predict(X_train.drop('y', axis=1))\n",
    "scores_rmse_train = log_link_back_trans(scores_rmse_train)\n",
    "\n",
    "scores_rmse_test = lgbmodel_rmse.predict(X_test.drop('y', axis=1))\n",
    "scores_rmse_test = log_link_back_trans(scores_rmse_test)\n",
    "\n",
    "scores_mae_train = lgbmodel_mae.predict(X_train.drop('y', axis=1))\n",
    "scores_mae_train = log_link_back_trans(scores_mae_train)\n",
    "\n",
    "scores_mae_test = lgbmodel_mae.predict(X_test.drop('y', axis=1))\n",
    "scores_mae_test = log_link_back_trans(scores_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the predictions at hand we finally compute the $r^{2}$ score and the realtive error of the global mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error train ghl =  0.0791477914076\n",
      "mean error test ghl  =  0.0697342566865\n",
      "mean error train mae =  0.153254702295\n",
      "mean error test mae =  0.143838062861\n",
      "mean error train rmse =  0.353968098928\n",
      "mean error test rmse =  0.347008685339\n",
      "             ..........          \n",
      "r2 score train ghl =  0.439297282841\n",
      "r2 scores test ghl  =  0.423486917981\n",
      "r2 score train mae =  0.421378611685\n",
      "r2 score test mae =  0.410388710145\n",
      "r2 score train rmse =  0.321584801783\n",
      "r2 score test rmse =  0.314374615368\n"
     ]
    }
   ],
   "source": [
    "print(\"mean error train ghl = \",\n",
    "      (y_train.mean() - scores_ghl_train.mean()) / (y_train.mean()))\n",
    "print(\"mean error test ghl  = \",\n",
    "      (y_test.mean() - scores_ghl_test.mean()) / (y_test.mean()))\n",
    "print(\"mean error train mae = \",\n",
    "      (y_train.mean() - scores_mae_train.mean()) / (y_train.mean()))\n",
    "print(\"mean error test mae = \",\n",
    "      (y_test.mean() - scores_mae_test.mean()) / (y_test.mean()))\n",
    "print(\"mean error train rmse = \",\n",
    "      (y_train.mean() - scores_rmse_train.mean()) / (y_train.mean()))\n",
    "print(\"mean error test rmse = \",\n",
    "      (y_test.mean() - scores_rmse_test.mean()) / (y_test.mean()))\n",
    "print(\"             ..........          \")\n",
    "print(\"r2 score train ghl = \", r2_score(y_train, scores_ghl_train))\n",
    "print(\"r2 scores test ghl  = \", r2_score(y_test, scores_ghl_test))\n",
    "print(\"r2 score train mae = \", r2_score(y_train, scores_mae_train))\n",
    "print(\"r2 score test mae = \", r2_score(y_test, scores_mae_test))\n",
    "print(\"r2 score train rmse = \", r2_score(y_train, scores_rmse_train))\n",
    "print(\"r2 score test rmse = \", r2_score(y_test, scores_rmse_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
